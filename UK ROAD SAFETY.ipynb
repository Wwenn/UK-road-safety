{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e349e2",
   "metadata": {},
   "source": [
    "# What affects road safety in the UK?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c8014",
   "metadata": {},
   "source": [
    "**Import note book instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b97d04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'available_if' from 'sklearn.utils.metaestimators' (/Users/ww/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/metaestimators.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vl/p06xrmln4yd37bkx5dycy7th0000gn/T/ipykernel_90588/2986635937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_easy_ensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEasyEnsembleClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bagging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedBaggingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalancedRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/_easy_ensemble.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_random_state_docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mMAX_INT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/imblearn/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         Guillaume Lemaitre <g.lemaitre58@gmail.com>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# License: BSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VisualBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetaestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mavailable_if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m from .utils import (\n\u001b[1;32m     24\u001b[0m     \u001b[0mBunch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'available_if' from 'sklearn.utils.metaestimators' (/Users/ww/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/metaestimators.py)"
     ]
    }
   ],
   "source": [
    "# For data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import datetime as dt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import itertools  \n",
    "\n",
    "# For data acquisition\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import urllib.request\n",
    "import zipfile\n",
    "#import networkx as nx\n",
    "\n",
    "# For ploting\n",
    "#!pip install plotly_express\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly_express as px\n",
    "import matplotlib.patches as mpatches\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import plotly.graph_objs as go \n",
    "import seaborn as sns \n",
    "# pip install geopy\n",
    "from geopy.distance import great_circle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#from plotly.subplots import make_subplots\n",
    "#from plotly.offline import init_notebook_mode \n",
    "from sklearn.cluster import DBSCAN\n",
    "import folium\n",
    "\n",
    "# For predicting with Randomforest and SOMTE\n",
    "from sklearn.ensemble import RandomForestClassifier # import classifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix # import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score # import evaluation tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83660ac1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baeb4c6e",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f4858",
   "metadata": {},
   "source": [
    "Traffic accidents have the most significant impact on mortality rates other than human diseases. There are some facts according to brake.org:\n",
    "\n",
    "**More than 1.35 million people die on the world's roads, and millions more are seriously injured every year.**\n",
    "\n",
    "**Road deaths are the eighth highest cause of death for people of all ages.**\n",
    "\n",
    "**Road deaths are the number one killer of those between the ages of 5-29.**\n",
    "\n",
    "In the traditional mindset, female drivers are always more prone to traffic accidents because of their 'poor' driving skills. But is this the case? Are young people more likely to be involved in traffic accidents than middle-aged people? When are traffic accidents more likely to happen, at Night or during the day? In which city or clusters in the UK are traffic accidents most likely to occur? How about the weather, road, and junction conditions.\n",
    "\n",
    "In this project, we will answer all of these questions. We hope to find the characteristics, spatial and temporal distribution, and future trends of traffic accidents by exploring the Road Safety database with data analysis and visualization. \n",
    "\n",
    "The advent of covid-19 was a huge disaster for humanity, and many people lost their lives and got sick due to covid-19, but it also changed human travel habits - reducing traffic trips and increasing the time spent at home. Therefore, we will further analyze the relationship between epidemics and road safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b6369",
   "metadata": {},
   "source": [
    "#  2. Dataset \n",
    "## 2.1 Introduction and Description of Dataset\n",
    "In this project, we need 2 kinds of dataset, one is dataset relating to road safety, the other is about covid-19.\n",
    "\n",
    "**Road Safety Data**\n",
    "\n",
    "Road Safety Dataset is published by Department for Transport((https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data), last updated on 25 November 2021 with an open government license.\n",
    " \n",
    "The dataset provides detailed road safety data about the circumstances of personal injury road accidents in GB from 2016, the types of vehicles involved, and the consequential casualties in 3 sub-dataset - Accident dataset, Casualty dataset, and Vehicles dataset. The statistics relate only to personal injury accidents on public roads reported to the police and subsequently recorded using the STATS19 accident reporting form.\n",
    "\n",
    "\n",
    "**Covid Data**\n",
    "\n",
    "The Covid-19 dataset to be analyzed in this project from ( https://coronavirus.data.gov.uk), published by the UK government, last updated on 20 January 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00317ee",
   "metadata": {},
   "source": [
    "## 2.2 Data Acquisition\n",
    "\n",
    "**Road Safety**\n",
    "\n",
    "We mainly use the urllib library to manipulate web page URLs and to read the content of web pages, then save the data we read in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the road safety data from 2016-2020 using urls\n",
    "URLs = [\"https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-accident-last-5-years.csv\",\n",
    "        \"https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-casualty-last-5-years.csv\",\n",
    "        \"https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-vehicle-last-5-years.csv\",\n",
    "       \"https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-accident-provisional-mid-year-unvalidated-2021.csv\"]\n",
    "\n",
    "for i,url in enumerate(URLs):\n",
    "    filename = f'df_{i}.csv'\n",
    "    urllib.request.urlretrieve(url,filename)\n",
    "    \n",
    "df1 = pd.read_csv('df_0.csv') # accident\n",
    "df2 = pd.read_csv('df_1.csv') # casualty\n",
    "df3 = pd.read_csv('df_2.csv') # vehicle\n",
    "df1_2021 = pd.read_csv('df_3.csv')# road safety data in 2021H1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a6fd6",
   "metadata": {},
   "source": [
    "**Covid Data**\n",
    "\n",
    "We used the API created by the gov.uk website to access their dataset on covid-19, find out the covid-19 information for each day, and select what we needed: newcase and mortality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Covid-19 Data \n",
    "from requests import get\n",
    "\n",
    "def get_data(url):\n",
    "    response = get(endpoint, timeout=10)   \n",
    "    if response.status_code >= 400:\n",
    "        raise RuntimeError(f'Request failed: { response.text }')\n",
    "        \n",
    "    return response.json()    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    endpoint = (\n",
    "        'https://api.coronavirus.data.gov.uk/v1/data?'\n",
    "        'filters=areaType=nation;areaName=england&'\n",
    "        'structure={\"date\":\"date\", \"confirmedNew\":\"newCasesByPublishDate\",\"confirmed\":\"cumCasesByPublishDate\", \"deathNew\":\"newDeaths28DaysByPublishDate\",\"death\":\"cumDeaths28DaysByPublishDate\",\"latestBy\":\"newCasesByPublishDate\"}'\n",
    "    )\n",
    "    \n",
    "covid_data = get_data(endpoint)\n",
    "#display(covid_data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d93d4",
   "metadata": {},
   "source": [
    "##  2.3 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de4d17",
   "metadata": {},
   "source": [
    "### 2.3.1 Initial Data Cleaning\n",
    "\n",
    "**Road Safety**\n",
    "\n",
    "To simplify the analysis of the data that follows and to improve the efficiency of the analysis, we do the data cleaning in the following steps:\n",
    "\n",
    "1. Drop unwanted columns from 3 types of datasets relating to road safety.\n",
    "2. Merge these 3 datasets and drop duplicate columns.\n",
    "3. Check the dataset for NA values.\n",
    "\n",
    "Different data visualisations require different versions of the data framework. Initial data cleaning is done in this section, further ad hoc data p grouping and other cleaning methods will be carried out throughout the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select wanted columns for df1,df2 and df3\n",
    "\n",
    "df1 = df1[['accident_index','accident_reference','accident_year','accident_severity','longitude','latitude','number_of_vehicles','number_of_casualties',\n",
    "          'date','day_of_week','time','road_type','speed_limit','junction_control','junction_detail','light_conditions','weather_conditions'\n",
    "           ,'local_authority_highway','local_authority_district']]\n",
    "\n",
    "df2 = df2[['accident_index','accident_reference', 'accident_year','casualty_reference','age_band_of_casualty','age_of_casualty','sex_of_casualty','casualty_severity',\n",
    "          'casualty_type']]\n",
    "\n",
    "df3 = df3[['accident_index', 'accident_reference','accident_year','age_of_driver','age_band_of_driver','sex_of_driver','junction_location',\n",
    "          'vehicle_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Missing Values\n",
    "print('Proportion of Missing Values in Accidents Table:', \n",
    "      round(df1.isna().sum().sum()/len(df1), 3), '%')\n",
    "print('Proportion of Missing Values in Casualty Table:', \n",
    "      round(df2.isna().sum().sum()/len(df2), 3), '%')\n",
    "print('Proportion of Missing Values in Vehicle Table:', \n",
    "      round(df3.isna().sum().sum()/len(df3), 3), '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe90267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN to -1\n",
    "df1.fillna(-1,inplace=True)\n",
    "df1.isna().sum().sum()\n",
    "print('Proportion of Missing Values in Accidents Table(After):', \n",
    "      round(df1.isna().sum().sum()/len(df1), 3), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge date\n",
    "df=pd.merge(df1,df2,how=\"right\")\n",
    "df=pd.merge(df,df3,how=\"right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15820ea9",
   "metadata": {},
   "source": [
    "**Covid Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "co19=pd.DataFrame(covid_data['data'])\n",
    "covid=co19.dropna() #Drop na covid data\n",
    "print('Proportion of Missing Values in Covid Table):', \n",
    "      round(covid.isna().sum().sum()/len(covid), 3), '%')\n",
    "#covid.head()\n",
    "covid = covid.loc[covid['date'] < '2021-12-02'] # we select the covid data whose data before 2021-12-02, because we analyzed traffic accidents before December 1, 2021 only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7de03e",
   "metadata": {},
   "source": [
    "### 2.3.2 Reshape Data\n",
    "\n",
    "**Road Safety**\n",
    "\n",
    "Many data in the road safety dataset are an index presented by a number, such as 'sex_of_driver = 1' means male, 'weather condition = 4' means rain, etc. So to show more information directly in the following visualization, we have renamed some of the data, taking reference from the 'Guide Index' provided by the UK Transport Department.\n",
    "\n",
    "We use a for loop to first change all types of the column we need to strings for facilitating the replacement of numeric indicators with text. Secondly, as dates and times are relatively special variables, we need to convert them into data time objects. Also, to facilitate the subsequent data analysis, we merged and reclassified the time variables, dividing the 24 hours of the day into five phases: morning rush, office hours, afternoon rush, evening and Night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert object to string\n",
    "for col in ['sex_of_casualty', 'sex_of_driver','accident_severity','age_band_of_casualty', 'age_band_of_driver',\n",
    "            'road_type','weather_conditions','casualty_severity','junction_detail','junction_control','light_conditions']:\n",
    "    df[col] = df[col].apply(str)\n",
    "\n",
    "# Replace new columns\n",
    "df['casualty_severity'].replace(['1.0','2.0','3.0'],\n",
    "                                   ['Fatal','Serious','Slight'],inplace=True)                                \n",
    "\n",
    "df['accident_severity'].replace(['1.0','2.0','3.0'],\n",
    "                                   ['Fatal','Serious','Slight'],inplace=True)                                \n",
    "\n",
    "df['road_type'].replace(['-1.0','1.0','2.0','3.0','6.0','7.0','9.0','12.0'],['Data missing or out of range',\n",
    "                                                                              'Roundabout',\n",
    "                                                                              'One way street',\n",
    "                                                                              'Dual carriageway',\n",
    "                                                                              'Single carriageway',\n",
    "                                                                              'Slip road',\n",
    "                                                                              'Unknown',\n",
    "                                                                              'One way street/Slip road'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df['age_band_of_driver'].replace(['1','2','3','4','5','6','7','8','9','10','11','-1'],\n",
    "                                   ['0-5','6-10','11-15','16-20','21-25','26-35','36-45','46-55','56-65','66-75','Over 75','Data missing']\n",
    "                                 ,inplace=True)\n",
    "\n",
    "df['age_band_of_casualty'].replace(['1.0','2.0','3.0','4.0','5.0','6.0','7.0','8.0','9.0','10.0','11.0','-1.0'],\n",
    "                                   ['0-5','6-10','11-15','16-20','21-25','26-35','36-45','46-55','56-65','66-75','Over 75','Data missing']\n",
    "                                   ,inplace=True)\n",
    "\n",
    "df['sex_of_casualty'].replace(['1.0','2.0','9.0','-1.0'],['Male',\n",
    "                                                          'Female',\n",
    "                                                          'Unknown',\n",
    "                                                          'Data missiong']\n",
    "                                                                ,inplace=True)\n",
    "df['sex_of_driver'].replace(['1','2','3','-1'],['Male',\n",
    "                                                          'Female',\n",
    "                                                          'Unknown',\n",
    "                                                          'Data missiong']\n",
    "                                                                ,inplace=True)\n",
    "\n",
    "\n",
    "df['junction_detail'].replace(['-1.0','0.0','1.0','2.0','3.0','5.0','6.0','7.0','8.0','9.0','99.0'],['NoData','Not at junction or within 20 metres','Roundabout',\n",
    "                                                           'Mini-roundabout','T or staggered junction','Slip road','Crossroads',\n",
    "                                                           'More than 4 arms (not roundabout)','Private drive or entrance'\n",
    "                                                           ,'Other junction','NoData2']\n",
    "                                                                ,inplace=True)\n",
    "\n",
    "\n",
    "df['junction_control'].replace(['0.0','1.0','2.0','3.0','4.0','-1.0','9.0'],['NoData','Not at junction or within 20 metres','Authorised person',\n",
    "             'Auto traffic signal','Stop sign', 'Give way or uncontrolled','NoData2']\n",
    "                                                                ,inplace=True)\n",
    "\n",
    "df['light_conditions'].replace(['1.0','4.0','5.0','6.0','7.0','-1.0'], \n",
    "                               [\"Daylight\",\"Darkness - lights lit\",\"Darkness - lights unlit\",\n",
    "                                \"Darkness - no lighting\",\"Darkness - lighting unknown\",\"Unknown\"],\n",
    "                               inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c6e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert date to datetime type\n",
    "df['date']=pd.to_datetime(df['date'])\n",
    "#Classify time and redefine a new column\n",
    "df['Hour'] = df['time'].str[0:2]\n",
    "\n",
    "# convert new column to numeric datetype\n",
    "df['Hour'] = pd.to_numeric(df['Hour'])\n",
    "\n",
    "# drop null values in our new column\n",
    "df = df.dropna(subset=['Hour'])\n",
    "# cast to integer values\n",
    "df['Hour'] = df['Hour'].astype('int')\n",
    "\n",
    "def when_was_it(hour):\n",
    "    if hour >= 5 and hour < 10:\n",
    "        return \"Morning rush (5-10)\"\n",
    "    elif hour >= 10 and hour < 15:\n",
    "        return \"Office hours (10-15)\"\n",
    "    elif hour >= 15 and hour < 19:\n",
    "        return \"Afternoon rush (15-19)\"\n",
    "    elif hour >= 19 and hour < 23:\n",
    "        return \"Evening (19-23)\"\n",
    "    else:\n",
    "        return \"Night (23-5)\"\n",
    "    \n",
    "df['Daytime'] = df['Hour'].apply(when_was_it)\n",
    "df.drop(df[(df['Daytime'] == 'data missing')]\\\n",
    "                     .index, axis=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a98211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check finaly the missing values in df\n",
    "print('Proportion of Missing Values in Road Safety:', \n",
    "      round(df.isna().sum().sum()/len(df), 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5eabce",
   "metadata": {},
   "source": [
    "**ARIMA Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the road safety data in 2020 and 2021\n",
    "a_2021 = df1_2021.pivot_table(values=[\"accident_reference\"], index=\"date\",aggfunc = 'count')\n",
    "a_2020 = df1.pivot_table(values=[\"accident_reference\"], index=\"date\",aggfunc = 'count')\n",
    "aa = pd.concat([a_2020,a_2021],axis=0) #aa means all the data of accidents\n",
    "\n",
    "# sort the date in an ascending order\n",
    "aa.reset_index(level=None, drop=False, inplace=True)\n",
    "aa['date'] = pd.to_datetime(aa['date'])\n",
    "aa = aa.sort_values(by='date')\n",
    "aa.columns = ['Date','Accidents']\n",
    "\n",
    "# select all of the accidents before 2020-03-15, when the covid outbreak in UK\n",
    "aa_bc = aa.loc[aa['Date'] < '2020-03-15']\n",
    "aa_bc = aa_bc.set_index(['Date'],drop=True)\n",
    "print('Proportion of Missing Values :', \n",
    "      round(aa_bc.isna().sum().sum()/len(aa_bc), 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38304d",
   "metadata": {},
   "source": [
    "# 3.Data analysis and visualisation\n",
    "\n",
    "##  3.1 When do traffic accidents usually happen?\n",
    "\n",
    "To examine the high incidence of traffic accidents over the period 2016-2020, we first analyzed the change in the number of traffic accidents over the period, every month (monthly and every three months). We then explicitly analyzed which weekdays and periods had the highest traffic accidents.\n",
    "\n",
    "**Accidents per month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7631a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "# plot\n",
    "df.set_index('date').resample('M').size().plot(label='Total per Month', color='grey', ax=ax)\n",
    "df.set_index('date').resample('M').size().rolling(window=3).mean()\\\n",
    "                           .plot(color='darkorange', linewidth=5, label='3-Months Moving Average', ax=ax)\n",
    "\n",
    "ax.set_title('Accidents per Month', fontsize=14, fontweight='bold')\n",
    "ax.set(ylabel='Total Count\\n', xlabel='')\n",
    "ax.legend(bbox_to_anchor=(1.1, 1.1), frameon=False)\n",
    "\n",
    "# remove all spines\n",
    "sns.despine(ax=ax, top=True, right=True, left=True, bottom=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f91913",
   "metadata": {},
   "source": [
    "**The end of each year is a high accident period and the total number of traffic accidents decreases each year**\n",
    "\n",
    "As the graph shows, the high incidence of traffic accidents is concentrated at the end of each year, with a downward trend in the number of accidents from the end of the year to the beginning of the following year. This suggests that the increase or decrease in traffic accidents is likely to be related to the holidays at the end of each year. At the same time, the overall trend is that the accidents are decreasing year on year. This indicates that the overall safety situation on the roads is gradually improving.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb5c1b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Accidents by Times and Weekdays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc061a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define day_of_week\n",
    "df['day_of_week']=df['date'].dt.day_name()\n",
    "\n",
    "\n",
    "weekday_counts = pd.DataFrame(df.set_index('date').resample('1d')['accident_index'].size().reset_index())\n",
    "weekday_counts.columns = ['date', 'Count']\n",
    "weekday_counts['day_of_week']=weekday_counts['date'].dt.day_name()\n",
    "weekday = weekday_counts['date'].dt.day_name()\n",
    "\n",
    "# Calculate average accident in weekday\n",
    "weekday_averages = pd.DataFrame(weekday_counts.groupby(weekday)['Count'].mean().reset_index())\n",
    "weekday_averages.columns = ['Weekday', 'Average_Accidents']\n",
    "\n",
    "# Redefine new dataframe\n",
    "weekday = df['date'].dt.day_name()\n",
    "Daytime = df['Daytime']\n",
    "\n",
    "\n",
    "accident_table = df.groupby([Daytime, weekday]).size()\n",
    "accident_table = accident_table.rename_axis(['Daytime', 'Weekday'])\\\n",
    "                               .unstack('Weekday')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(accident_table,cmap=\"YlGnBu\")\n",
    "plt.title('\\nAccidents by Times and Weekdays\\n', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b73ba",
   "metadata": {},
   "source": [
    "**The afternoon rush and office hours are the most accident-prone times of the day, with the Friday afternoon peak often being the most accident-prone time of the week.**\n",
    "\n",
    "The heat map shows significantly fewer crashes during the afternoon rush hour on weekends than on weekdays, while there are substantially more crashes on weekend evenings than on weekdays. More people go out in the evenings on weekends, and people have more nightlife than on weekdays, so there are more traffic accidents at Night on weekends than on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6062c90",
   "metadata": {},
   "source": [
    "##  3.2 Who is more likely to be involved in a traffic accident?\n",
    "In this section, we depicted portraits of drivers and casualties in the traffic accident. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabced8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new dataset about driver and caculate accident number\n",
    "drivers = df.groupby(['age_band_of_driver', 'sex_of_driver']).size().reset_index()\n",
    "drivers['Type']='Driver'\n",
    "drivers.columns = ['Age_Band', 'Sex', 'Count','Type']\n",
    "\n",
    "# Get a new dataset about casualty and caculate accident number\n",
    "casualty = df.groupby(['age_band_of_casualty', 'sex_of_casualty']).size().reset_index()\n",
    "casualty['Type']='Causualty'\n",
    "casualty.columns = ['Age_Band', 'Sex', 'Count','Type']\n",
    "\n",
    "# Merge date\n",
    "aas = pd.concat([casualty, drivers], axis=0)\n",
    "\n",
    "aas['index']=list(range(0,78,1))\n",
    "aas.set_index('index', inplace=True)\n",
    "\n",
    "# Data clean\n",
    "aas.reset_index(level=None, drop=False, inplace=True)\n",
    "aas.drop(aas[(aas['Age_Band'] == 'Data missing') | \\\n",
    "                     (aas['Sex'] == 'Unknown') | \\\n",
    "                     (aas['Sex'] == 'unknow') | \\\n",
    "                     (aas['Sex'] == 'Data missing')| \\\n",
    "                     (aas['Sex'] == 'Data missiong')]\\\n",
    "                     .index, axis=0, inplace=True)\n",
    "#display(aas)\n",
    "\n",
    "# Plot\n",
    "fig = px.bar(aas, x=\"Age_Band\", y=\"Count\", color=\"Type\", barmode=\"group\", facet_col=\"Sex\",\n",
    "            category_orders={\"Age_Band\": [\"0-5\", \"6-10\",\"11-15\", \"16-20\", \"21-25\",\n",
    "                                                    \"26-35\",\"36-45\",\"46-55\",\"56-65\",\"66-75\",\"Over 75\"],\n",
    "                             \"Sex\": [\"Male\", \"Female\"]})\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"images/fig2.jpeg\")\n",
    "# Caculate\n",
    "# https://www.statista.com/statistics/281240/population-of-the-united-kingdom-uk-by-gender/\n",
    "# https://www.statista.com/statistics/314886/percentage-of-adults-holding-driving-licences-england/\n",
    "sod = df.groupby(['sex_of_driver']).size().reset_index()\n",
    "sod.loc[sod['sex_of_driver']=='Female']\n",
    "f_2 = 352527/(33.94*1000000*0.70)\n",
    "print('probability of a female driver being involved in a crash:'+'percent: {:.2f}%'.format(f_2*100))\n",
    "\n",
    "sod.loc[sod['sex_of_driver']=='Male']\n",
    "f_3 = 788352/(33.15*1000000*0.81)\n",
    "print('probability of a male driver being involved in a crash:'+'percent: {:.2f}%'.format(f_3*100))\n",
    "\n",
    "soc = df.groupby(['sex_of_casualty']).size().reset_index()\n",
    "soc_p = soc.iloc[2,1]/(soc.iloc[2,1]+soc.iloc[1,1])\n",
    "print('percentage of male casualties:'+'percent: {:.2f}%'.format(soc_p*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ca114b",
   "metadata": {},
   "source": [
    "**Males are more likely to be involved in traffic accidents than females. Male drivers are twice as likely as female drivers to be involved in a traffic accident. 70% of the casualties were men.** \n",
    "\n",
    "In 2020, the population of the United Kingdom was over 67 million, with 33.94 million females and 33.15 million males, which means that the number of males and females is well balanced in the UK. According to Statista, there is an 11% gap, with 81% of men holding a licence and 70% women. Therefore, we can calculate that the probability of a female driver being involved in a crash is 1.5%(352527/[33.94million*70%]), compared to 2.9%(788352/[33.15million*81%]) for male drivers over the past five years.\n",
    "\n",
    "As for casualties, the number of male casualties was 726,276( 59.12%) and 502,263 female casualties. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff494e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new dataset about driver and casualty \n",
    "drivers2 = df.groupby(['age_of_driver']).size().reset_index()\n",
    "casualty2 = df.groupby(['age_of_casualty']).size().reset_index()\n",
    "drivers2['Type'] = 'Driver'\n",
    "casualty2['Type'] = 'Casualty'\n",
    "\n",
    "# Rename column\n",
    "drivers2.columns = ['Age','Number','Type']\n",
    "casualty2.columns = ['Age','Number','Type']\n",
    "\n",
    "# Merge data\n",
    "age = pd.concat([drivers2, casualty2], axis=0)\n",
    "\n",
    "# Drop missing value(-1)\n",
    "age = age.loc[age['Age'] != -1]\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure()\n",
    "fig = px.scatter(age, x=\"Age\", y=\"Number\", color='Type',marginal_x=\"histogram\", marginal_y=\"rug\")\n",
    "fig.update_layout(xaxis=dict(rangeslider=dict(visible=True)))# Add range slider\n",
    "fig.update_layout(title='Age distribution of traffic accident driver and casualty')\n",
    "fig.show()\n",
    "fig.write_image(\"images/fig.jpeg\")\n",
    "\n",
    "# Caculate\n",
    "poa1 = age.loc[age['Age']==16]\n",
    "poa2 = age.loc[age['Age']==18]\n",
    "\n",
    "poa3 = (poa2.iloc[0,1]-poa1.iloc[0,1])/poa1.iloc[0,1]\n",
    "poa4 = (poa2.iloc[1,1]-poa1.iloc[1,1])/poa1.iloc[1,1]\n",
    "print('Multiplier for the increase in the number of drivers involved:',poa3)\n",
    "print('Percentage increase in the number of traffic casualties between the ages of 16 and 18:'+'percent: {:.2f}%'.format(poa4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7299830",
   "metadata": {},
   "source": [
    "**Thirty-year-olds are most likely to be involved in traffic accidents and become victims. Young people who have just received their driver's licence need to stay alert.**\n",
    "\n",
    "The age of 16-18 and 30 are the two cut-off points that signal a significant increase and decrease in the likelihood of a traffic accident, respectively. \n",
    "\n",
    "The number of traffic accidents and casualties increased significantly between the ages of 16-18, with the number of casualties rising from 13.39k to 28.64k, an increase of **114%**, and the number of drivers involved increasing from 3.98k to 22.3k, an increase of **4.6** times. Because young people in the UK can take the test to obtain a legal driver's licence at age 16, the increase in the sample base also led to a significant increase in the number of 16-18 year old drivers involved in traffic accidents. At the same time, 18-year-olds in the UK can drink alcohol legally. Drunk driving is the leading cause of traffic accidents and exacerbates their severity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09e16f",
   "metadata": {},
   "source": [
    "## 3.3 Under what circumstances are traffic accidents more likely to occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0331593",
   "metadata": {},
   "source": [
    "### 3.3.1 Weather Condition: accidents in foggy days are more severe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cb0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=df1.replace(0,np.nan)\n",
    "weather = df1.pivot_table(values=\"accident_index\", index=\"accident_severity\", columns=\"weather_conditions\",aggfunc = 'count')\n",
    "weather.columns = ['Data missing','Fine no high winds',\n",
    "                   'Raining no high winds','Snowing no high winds',\n",
    "                   'Fine + high winds','Raining + high winds',\n",
    "                   'Snowing + high winds','Fog or mist','Other','Unknown']\n",
    "weather.drop(['Data missing','Unknown'],axis=1, inplace=True)\n",
    "weather.reset_index(level=None, drop=False, inplace=True)\n",
    "weather.set_index('accident_severity', inplace=True)\n",
    "weather2 = weather\n",
    "w = weather.T.values.tolist()\n",
    "w2 = weather2.T.values.tolist()\n",
    "\n",
    "for i in range(6,8):\n",
    "    sum = w[i][1]+w[i][2]+w[i][0]\n",
    "    w[i][0]=w[i][0]/sum\n",
    "    w[i][1]=w[i][1]/sum\n",
    "    w[i][2]=w[i][2]/sum\n",
    "\n",
    "for i in range(0,3):\n",
    "    for j in range(0,3):\n",
    "        w[i][j]=w[i][j]+w[i+3][j]      \n",
    "    sum = w[i][1]+w[i][2]+w[i][0]\n",
    "    w[i][0]=w[i][0]/sum\n",
    "    w[i][1]=w[i][1]/sum\n",
    "    w[i][2]=w[i][2]/sum\n",
    "\n",
    "for i in [0,3]:\n",
    "    for j in range(0,3):\n",
    "        w2[i][j]=w2[i][j]+w2[i+1][j]+w2[i+2][j]    \n",
    "    sum = w2[i][1]+w2[i][2]+w2[i][0]\n",
    "    w2[i][0]=w2[i][0]/sum\n",
    "    w2[i][1]=w2[i][1]/sum\n",
    "    w2[i][2]=w2[i][2]/sum\n",
    "\n",
    "weather = pd.DataFrame(w).T\n",
    "weather2 = pd.DataFrame(w2).T\n",
    "weather.columns = ['Fine','Raining','Snowing',\n",
    "                   'Fine + high winds','Raining + high winds','Snowing + high winds',\n",
    "                   'Fog or mist','Other']\n",
    "weather2.columns = ['No winds','Raining','Snowing',\n",
    "                   'High Winds','Raining + high winds',\n",
    "                   'Snowing + high winds','Fog or mist','Other']\n",
    "weather = weather.T\n",
    "weather2 = weather2.T\n",
    "weather.drop(['Fine + high winds','Raining + high winds','Snowing + high winds','Other'],axis=0, inplace=True)\n",
    "weather2.drop(['Raining','Snowing',\n",
    "               'Raining + high winds','Snowing + high winds',\n",
    "               'Fog or mist','Other'],axis=0, inplace=True)\n",
    "\n",
    "weather=pd.concat([weather, weather2], axis=0)\n",
    "weather.reset_index(level=None, drop=False, inplace=True)\n",
    "weather.columns = ['Weather','Fatal','Serious','Slight']\n",
    "#display(weather)\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    go.Bar(x=[[\"Weather\",\"Weather\",\"Weather\",\"Weather\",\"Wind\",\"Wind\"],\n",
    "              ['Fine','Raining','Snowing','Fog or mist','No winds','High Winds']],\n",
    "           y=weather['Fatal'], \n",
    "           name='Fatal'))\n",
    "fig.add_trace(go.Bar(x=[[\"Weather\",\"Weather\",\"Weather\",\"Weather\",\"Wind\",\"Wind\"],\n",
    "              ['Fine','Raining','Snowing','Fog or mist','No winds','High Winds']],\n",
    "                     y=weather['Serious'], name='Serious'))\n",
    "fig.add_trace(go.Bar(x=[[\"Weather\",\"Weather\",\"Weather\",\"Weather\",\"Wind\",\"Wind\"],\n",
    "              ['Fine','Raining','Snowing','Fog or mist','No winds','High Winds']],\n",
    "                     y=weather['Slight'], name='Slight'))\n",
    "\n",
    "fig.update_layout(barmode='stack',title_text=\"The severity of traffic accidents in different weather\")\n",
    "fig.update_xaxes(showgrid=True, ticks=\"outside\", tickson=\"boundaries\",categoryorder='array')\n",
    "fig.update_yaxes(showgrid=True, ticks=\"outside\", tickson=\"boundaries\",categoryorder='array')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5450f5c",
   "metadata": {},
   "source": [
    "**Serious and fatal accidents are more likely to occur on foggy or mist and high winds days. Serious traffic accidents are more likely to occur on sunny days than on rainy or snowy days.**\n",
    " \n",
    "Here, we divide weather conditions into two categories: meteorological and wind. As expected, severe or fatal traffic accidents are more likely to occur on fog or mist days in meteorological conditions. However, contrary to our expectations, the number of accidents on fine days is higher than on rainy and snowy days. The accidents are often more severe, with a percentage of 20%, 19%, and 16% each. That's because, on sunny days, people tend to be subjectively more careless, getting distracted by scenery nearby. In contrast, people will be more careful on rainy and snowy days at a lower driving speed in a relaxed mood.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6e275",
   "metadata": {},
   "source": [
    "### 3.3.2 Road Condition: \n",
    "20, 30, 40, 50, 60, 70 are the only valid speed limits on public highways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new dataset about road condition and calculate accident \n",
    "ss1 = pd.pivot_table(df,index=['road_type','light_conditions','speed_limit'],\n",
    "                values=['number_of_vehicles','number_of_casualties'],aggfunc='sum')\n",
    "ss1=ss1.reset_index()\n",
    "\n",
    "# Data clean\n",
    "ss1.drop(ss1[(ss1['road_type'] == 'Unknown') | \\\n",
    "                     (ss1['light_conditions'] == 'Unknown')]\\\n",
    "                     .index, axis=0, inplace=True)\n",
    "\n",
    "# Plot\n",
    "fig1 = px.scatter_polar(ss1, r=\"speed_limit\", theta=\"road_type\",\n",
    "                      color=\"light_conditions\", size=\"number_of_casualties\",\n",
    "                      color_discrete_sequence=px.colors.sequential.Plasma_r)\n",
    "fig1.show()\n",
    "\n",
    "ll = df.pivot_table(values=\"accident_index\", index=[\"light_conditions\",\n",
    "                                                    \"accident_severity\"],aggfunc = 'count')\n",
    "\n",
    "ll['percentage%']=ll.groupby(level=0).apply(lambda x:100*x/x.sum())\n",
    "\n",
    "ll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4433c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new dataset about road type\n",
    "road = df.groupby('road_type').size()\\\n",
    "                                           .reset_index(name='counts')# calculate accident count\n",
    "\n",
    "# Data clean\n",
    "road.drop(road[(road['road_type'] == 'Data missing or out of range') | \\\n",
    "                     (road['road_type'] == 'Unknown')]\\\n",
    "                     .index, axis=0, inplace=True)\n",
    "\n",
    "\n",
    "road=road.set_index('road_type')\n",
    "from random import shuffle\n",
    "\n",
    "# Plot\n",
    "slices = [10,20,30,40]*2  \n",
    "shuffle(slices)\n",
    "cmap = plt.cm.Pastel1\n",
    "colors = cmap(np.linspace(0., 1., len(slices)))\n",
    "\n",
    "road.plot.pie(figsize=(8, 8),colors=colors,autopct='%1.1f%%',subplots=True, shadow = False,\n",
    "              title='Road type of Accident')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f200c2",
   "metadata": {},
   "source": [
    "**The dual carriageway is more prone to fatal traffic accidents. Nighttime lighting reduces the severity of traffic accidents. Traffic accidents get more severe when they exceed a higher speed limit**\n",
    "\n",
    "A single-carriageway has more traffic accidents than a dual carriageway among the different road types. Of the 91,726 traffic accidents on the dual carriageway, 1.73% were fatal accidents with the highest percentage. In a roundabout, the probability of having a severe traffic accident is 0.36%. Because there are vehicles from both ends of the road in the dual carriageway, and when two cars are crashing towards each other, it is easy to produce a strong rush impact. However, in a single carriageway, the possibility of a slight accident is the least (79.3%); however, in slip road, a slight accident is the most (90.2%). This is because speeds tend to be higher on the single carriageway, while speed limits are set on the slip road. Reducing the driving speed will reduce the accident's impact, thus mitigating the accident.\n",
    "\n",
    "Of the 20,744 accidents in the no lighting environment, 4.75% were severe, compared with 1.1% of the 428,051 accidents in daylight. 72% of accidents occurred during the day because more people and cars travel during the day; 20% of the number of accidents occurred in the unlit environment. Therefore, the severity of traffic accidents can be reduced by adjusting the brightness of lights at Night.\n",
    "\n",
    "60% of traffic accidents occur on roads with a speed limit of 30miles/hour, and 13 per cent of traffic accidents occur on roads with a speed limit of 60miles/hour. Because roads with speed limits of 30miles/hour are often sharp curves, broken roads and arch bridges, drivers and pedestrians are often too inflexible to make new judgments in these places with special topography and are more likely to have traffic accidents. Suppose traffic accidents occur in areas with higher speed limits. In that case, traffic accidents tend to be more serious; the proportion of serious traffic accidents with a speed limit of 60 miles/hour is 3.6%. In traffic accidents with a speed limit of 20, the ratio of serious is only 0.48%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ba232",
   "metadata": {},
   "source": [
    "###  3.3.3 Junction Conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a new dataset about junction condition\n",
    "jc = df.pivot_table(values=\"accident_index\", index=[\"junction_detail\",\n",
    "                                                    \"junction_control\"],aggfunc = 'count')# caculate count of accident \n",
    "\n",
    "# Data cleaning\n",
    "jc=jc.reset_index()\n",
    "jc.drop(jc[(jc['junction_detail'] == 'NoData') | \\\n",
    "                     (jc['junction_detail'] == 'NoData2') | \\\n",
    "                     (jc['junction_control'] == 'Not at junction or within 20 metres') | \\\n",
    "                     (jc['junction_control'] == 'NoData')  | \\\n",
    "                     (jc['junction_control'] == 'NoData2')]\\\n",
    "                     .index, axis=0, inplace=True)\n",
    "\n",
    "#plot\n",
    "fig = px.bar_polar(jc, r=\"accident_index\", theta=\"junction_detail\", color=\"junction_control\", #line_close=True,\n",
    "                   color_discrete_sequence=px.colors.sequential.Plasma_r)\n",
    "                  \n",
    "fig.show()\n",
    "\n",
    "cc = df.pivot_table(values=\"accident_index\", index=[\"junction_detail\",\n",
    "                                                    \"accident_severity\"],aggfunc = 'count')\n",
    "\n",
    "cc['percentage%']=cc.groupby(level=0).apply(lambda x:100*x/x.sum())\n",
    "\n",
    "cc.head()          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf86387",
   "metadata": {},
   "source": [
    "**58% of accidents occur at intersections, 50% of which occur at T or staggered junction; stop signs and authorized persons can help reduce the number of accidents**\n",
    "\n",
    "58% of accidents occur at intersections, 42% of accidents not at junctions or within 20 metres. Intersections are hubs of road traffic, and traffic flows in different directions from more conflict points and interweaving points at intersections. Drivers are often distracted from driving at intersections because they think about their travel route. As a result, intersections tend to be a high incidence point for traffic accidents. Of all accidents, 30% occur at T or staggered junctions, 9.5% at crossroads, and 8.2% at roundabouts, so extra care is needed when approaching these two types of intersections. In the slip road, when driving downhill, the driver often takes the operation method of turning off the engine and coasting to save fuel. In an emergency, it is too late to take emergency measures.\n",
    "\n",
    "45% of accidents occur at give way or uncontrolled junctions, 11% at auto traffic signals, 0.6% at junctions with a stop sign, and only 0.3% at intersections with an authorized person. Therefore, to some extent, installing a stop sign at a junction can effectively prevent traffic accidents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc53f0",
   "metadata": {},
   "source": [
    "## 3.4 Spatial Analysis: In which cities or regions traffic accidents occur more frequently?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc4bce",
   "metadata": {},
   "source": [
    "### 3.4.1 Cities: London and Birmingham have the highest number of traffic accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the json fill\n",
    "u1 = open(\"Local_Authority_Districts_(December_2020)_UK_BFC.json\")\n",
    "uk_districts = json.load(u1)\n",
    "\n",
    "# import the index file from the Road Safety Open-Dataset-Data-Guide\n",
    "index = pd.read_excel(\"https://data.dft.gov.uk/road-accidents-safety-data/Road-Safety-Open-Dataset-Data-Guide.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417c8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning - index\n",
    "index2=index.loc[index['field name'] == \"local_authority_district\"]\n",
    "dis = df1.pivot_table(values=\"accident_index\", \n",
    "                          #index=\"accident_year\", \n",
    "                      columns=\"local_authority_district\",aggfunc = 'count')\n",
    "#dis = df[['accident_index','local_authority_district','accident_year']] \n",
    "#dis\n",
    "dis2 = dis.T\n",
    "dis2.reset_index(level=None, drop=False, inplace=True)\n",
    "dis3=pd.merge(dis2,index2,how=\"inner\",left_on=\"local_authority_district\",right_on=\"code/format\")\n",
    "#dis3\n",
    "\n",
    "#Here we can use the column \"label\" in the clean data and \"LAD20NM\" in the feature properties\n",
    "fig = px.choropleth_mapbox(dis3, locations=\"label\", featureidkey=\"properties.LAD20NM\", \n",
    "                           geojson=uk_districts, color=\"accident_index\", hover_name=\"label\", \n",
    "                           color_continuous_scale=px.colors.sequential.deep,\n",
    "                           mapbox_style=\"carto-positron\", zoom=4, center = {\"lat\": 55, \"lon\": 0})\n",
    "fig.show()\n",
    "fig.write_image(\"images/fig0.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fb255",
   "metadata": {},
   "source": [
    "By zooming in on the interactive map, we can see a black dot on the graph that represents the highest number of traffic accidents in this location in the last five years combined - 12.55k people. This is Birmingham, the second-largest city in the UK besides London. Because the number of accidents is divided by authority districts, it is impossible to show the enormous number of accidents in London as a whole. However, the number of accidents in London is the highest in the UK, as seen from the borough data alone. Meanwhile, Leeds, Cornwall and Wiltshire also have more traffic accidents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a386273",
   "metadata": {},
   "source": [
    "###  3.4.2 Cluster analysis : Locate accident hotspots in the Central London\n",
    "This section used clustering (DBSCAN) to identify accident hotspots in central London in 2020. DBSCAN groups point that are densely packed together and marks points outside of these groups as noise. Therefore, locations with high accident densities will be highlighted as clusters using this algorithm. We then use folium to plot the locations of these clusters. \n",
    "\n",
    "1. First, we need to filter the data we need.\n",
    "\n",
    "2. As DBSCAN needs to use an indicator when calculating the distance between points. We create a function that takes the latitude and longitude of two points and calculates the distance (in metres) between them.\n",
    "\n",
    "3. To ensure that the earth's curvature is taken into account when calculating the distance, we also created geopy's great_circle function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85521958",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster=df[['latitude','time','longitude','accident_year','date','number_of_vehicles','number_of_vehicles','local_authority_highway']]\n",
    "cluster=cluster[cluster['accident_year']==2020]\n",
    "City = cluster[cluster['local_authority_highway'] == 'E09000001']\n",
    "\n",
    "dfnew = pd.concat([City], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de49052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure that the curvature of the Earth is taken into account when calculating these distances\n",
    "def greatcircle(x,y):\n",
    "    lat1, long1 = x[0], x[1]\n",
    "    lat2, long2 = y[0], y[1]\n",
    "    dist = great_circle((lat1,long1),(lat2,long2)).meters\n",
    "    return dist\n",
    "# Determines the maximum distance between two samples for one to be considered as in the neighborhood of the other\n",
    "\n",
    "eps = 100 #distance in meters\n",
    "min_samples = 10\n",
    "\n",
    "df_dbc = dfnew\n",
    "\n",
    "loc = df_dbc[['latitude','longitude']]\n",
    "\n",
    "# DBSCAN groups points that are closely packed together and marks points outside of these groups as noise\n",
    "dbc = DBSCAN(eps = eps, min_samples = min_samples, metric=greatcircle).fit(loc)\n",
    "\n",
    "labels = dbc.labels_\n",
    "\n",
    "df_dbc['Cluster'] = labels\n",
    "\n",
    "location = df_dbc['latitude'].mean(), df_dbc['longitude'].mean()\n",
    "\n",
    "#plot\n",
    "m = folium.Map(location=location,zoom_start=13)\n",
    "\n",
    "folium.TileLayer('cartodbpositron').add_to(m)\n",
    "\n",
    "clust_colours = ['#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c','#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928']\n",
    "\n",
    "for i in range(0,len(df_dbc)):\n",
    "    colouridx = df_dbc['Cluster'].iloc[i]\n",
    "    if colouridx == -1:\n",
    "        pass\n",
    "    else:\n",
    "        col = clust_colours[colouridx%len(clust_colours)]\n",
    "        folium.CircleMarker([df_dbc['latitude'].iloc[i],df_dbc['longitude'].iloc[i]], radius = 10, color = col, fill = col).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372dbc77",
   "metadata": {},
   "source": [
    "**The traffic accident cluster is most remarkable near Liverpool Street Station and Monument Station in central London.**\n",
    "\n",
    "In city of London, there are five accident hotspots clusters, each shown in a different colour.\n",
    "\n",
    "From left to the right in the above figure, the light blue cluster includes the junction of Charterhouse St., West Smithfield, and Fairingdon St., near Holborn and LSE. The light green cluster is at the intersection of Londonwall and Moorgate. Pink cluster locates on the crossroads of Cannon St., Kings William St., and Grace Church St. Grass green cluster locates on the Bishopsgate St., Wormwood St., and Camomole St, near Liverpool St. Station, where the trend towards agglomeration is most evident. Navy cluster mainly locates on the Aldgate High St.\n",
    "\n",
    "All clusters are located in popular, crowded areas, especially near T-roads, intersections, and metro stations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84558b0",
   "metadata": {},
   "source": [
    "## 3.5 ARIMA: Covid accelerates reduction in the number of traffic accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a3a34",
   "metadata": {},
   "source": [
    "**Visualize Covid Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128c32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "# Create and style traces\n",
    "fig.add_trace(go.Scatter(x=covid['date'], y=covid['deathNew'], name = 'New death cases',\n",
    "                        line=dict(color='#FF9900', width=2)))\n",
    "fig.add_trace(go.Scatter(x=covid['date'], y=covid['latestBy'], name = 'Latest By',\n",
    "                        line=dict(color='#3366CC', width=2)))\n",
    "fig.add_trace(go.Scatter(x=covid['date'], y=covid['death'], name = 'Death cases',\n",
    "                        line=dict(color='#109618', width=2)))\n",
    "fig.add_trace(go.Scatter(x=covid['date'], y=covid['confirmed'], name = 'New confirmed cases',\n",
    "                        #mode='lines+markers',\n",
    "                        line=dict(color='#FECB52', width=2)))\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "   xaxis=dict(\n",
    "       rangeselector=dict(\n",
    "           buttons=list([\n",
    "               dict(count=1,\n",
    "                    label=\"1m\",\n",
    "                    step=\"month\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(count=6,\n",
    "                    label=\"6m\",\n",
    "                    step=\"month\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(count=1,\n",
    "                    label=\"YTD\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"todate\"),\n",
    "               dict(count=1,\n",
    "                    label=\"1y\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(step=\"all\")])),\n",
    "       rangeslider=dict(visible=True),\n",
    "       type=\"date\"))\n",
    "\n",
    "fig.update_layout(title='Covid data')\n",
    "fig.update_yaxes(title='Number')\n",
    "fig.update_xaxes(title='Date')\n",
    "fig.write_image(\"images/fig1.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d2c3b",
   "metadata": {},
   "source": [
    "**In March 2020, the Covid-19 epidemic began to spread in the UK.**\n",
    "\n",
    "Since March 2020, the number of new cases and deaths of the Covid-19 epidemic in the UK has started to rise. Some people have started to work from home and reduce their daily travel.\n",
    "\n",
    "**Tips:** \n",
    "After clicking on the new confirmed case in the legend, this curve will be cancelled. As a result, the images of the other three data sets will be more visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b2cf7",
   "metadata": {},
   "source": [
    "**Monthly Trend**\n",
    "\n",
    "**Step1:** Extract the average number of accidents in each month. Visualise the data using time-series decomposition that allows us to decompose our time series into three distinct components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f02dd2",
   "metadata": {},
   "source": [
    "This part aims to predict the number of future road accidents in the UK by implementing time Series forecasting methods  ARIMA. ARIMA stands for Auto-Regressive Integrated Moving Average. There are seasonal and Non-seasonal ARIMA models that can be used for forecasting. 3 terms characterize an ARIMA model: p, d, q where p is the order of the AR term, q is the order of the MA term, and d is the number of differences required to make the time series stationary. If a time series has seasonal patterns, you need to add seasonal terms, and it becomes SARIMA, short for Seasonal ARIMA. More on that once we finish ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = aa_bc['Accidents'].resample('MS').mean()\n",
    "rcParams['figure.figsize'] = 16, 10\n",
    "decomposition = sm.tsa.seasonal_decompose(y, model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5d67b4",
   "metadata": {},
   "source": [
    "**Step 2:** Fitting the ARIMA Model. Below are the examples of parameter combinations for seasonal ARIMA. This step is parameter Selection for our ARIMA Time Series Model. Our goal here is to use a grid search to find the optimal set of parameters that yields the best performance for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8458c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "#print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "#print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "#print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "#print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "#print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            #results = mod.fit()\n",
    "            \n",
    "            #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 1, 0, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "#print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5649c9",
   "metadata": {},
   "source": [
    "**Step3:** Evaluation of forecasts: In order to understand the accuracy of our forecasts, we compare predicted number of accidents to the real number of accidents of the time series, and we set forecasts to start at 20170101 to the end of the data. Find out the MSE to see the accuracy of our model. The mean squared error (MSE) is largely used as a metric to determine the performance of an algorithm. In addition, MSE is the average of the square of the difference between the observed and predicted values of a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e419ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start=pd.to_datetime('2019-01-01'), dynamic=False) \n",
    "pred_ci = pred.conf_int() \n",
    "ax = y['2016-01-01':].plot(label='observed') \n",
    "pred.predicted_mean.plot(ax=ax, label='forcast in advance', alpha=.7, figsize=(14, 7)) \n",
    "ax.fill_between(pred_ci.index, pred_ci.iloc[:, 0], pred_ci.iloc[:, 1], color='k', alpha=.2) \n",
    "ax.set_xlabel('date') \n",
    "ax.set_ylabel('Number of Accidents') \n",
    "ax.set_title('Traffic accident model fitting between 2019 and 2020')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8515e0e",
   "metadata": {},
   "source": [
    "The orange and blue lines overlap well with each other, representing a good model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7dd88",
   "metadata": {},
   "source": [
    "**Step 4:** Visualising Forecasts. We set the step to 21, means the model will predict the number of next 21 months. As we can see in the below graph the number of road accident in UK will be declined until the end of 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7758f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean\n",
    "y_truth = y['2019-01-01':]\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n",
    "\n",
    "pred_uc = results.get_forecast(steps=21) \n",
    "pred_ci = pred_uc.conf_int() \n",
    "ax = y.plot(label='observed', figsize=(14, 7)) \n",
    "pred_uc.predicted_mean.plot(ax=ax, label=' Forecast') \n",
    "ax.fill_between(pred_ci.index, \n",
    "                pred_ci.iloc[:, 0], \n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.25) \n",
    "ax.set_xlabel('Date') \n",
    "ax.set_ylabel('Accidents Number') \n",
    "ax.set_title('Prediction- Model fitting between 2020 and 2021')\n",
    "plt.legend() \n",
    "plt.show()\n",
    "a_p = pd.DataFrame(pred_uc.predicted_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a60533",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_p.reset_index(level=None, drop=False, inplace=True)\n",
    "a_p.columns = ['Date','predicted_mean']\n",
    "a_p['Date'] =a_p['Date'].apply(lambda x: pd.to_datetime(x).strftime('%Y-%m'))\n",
    "aa = aa.set_index(['Date'],drop=True)\n",
    "aa = aa.resample('M',closed = 'left').mean()\n",
    "aa.reset_index(level=None, drop=False, inplace=True)\n",
    "aa['Date'] =aa.Date.apply(lambda x: pd.to_datetime(x).strftime('%Y-%m'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "# Create and style traces\n",
    "fig.add_trace(go.Scatter(x=aa['Date'], y=aa['Accidents'], name='Actual Accident(Monthly)',\n",
    "                         mode='lines+markers',\n",
    "                        line=dict(color='firebrick', width=3)))\n",
    "fig.add_trace(go.Scatter(x=a_p['Date'], y=a_p['predicted_mean'], name = 'Predicted Accident(Monthly)',\n",
    "                         mode='lines+markers',\n",
    "                        line=dict(color='royalblue', width=3, dash='dot')))\n",
    "\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "   xaxis=dict(\n",
    "       rangeselector=dict(\n",
    "           buttons=list([         \n",
    "               dict(count=1,\n",
    "                    label=\"1y\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(count=2,\n",
    "                    label=\"2y\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(count=3,\n",
    "                    label=\"3y\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(step=\"all\")])),\n",
    "       rangeslider=dict(visible=True),\n",
    "       type=\"date\"))\n",
    "fig.update_layout(title='Monthly Actual Number and Predicted Number of Accidents')\n",
    "fig.update_yaxes(title='Number of Accidents')\n",
    "fig.update_xaxes(title='Date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d90a2",
   "metadata": {},
   "source": [
    "**After the occurrence of the Covid, the traffic accidents accelerated to decrease.**\n",
    "\n",
    "Here, we use traffic accident data from March 2020 (before the outbreak of Covid-19) as a sample and use an ARIMA model to predict and compare them with actual values.\n",
    "\n",
    "After the occurrence of the Covid, the traffic accidents accelerated to decrease. In the graph, all monthly forecasts (blue dashed line) are higher than the true values (solid red line). This is especially true for April 2020, February 2021, and October-November 2021. This is because strict quarantine policies enacted by the government or Omicron emerged and spread widely during this time, causing people to reduce their travel activities and thus reduce traffic accidents. The next graph shows the relationship between covid and traffic accident data more clearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4786876",
   "metadata": {},
   "source": [
    "**Daily Trend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b341c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = aa_bc['Accidents'].resample('D').mean()\n",
    "rcParams['figure.figsize'] = 16, 10\n",
    "decomposition = sm.tsa.seasonal_decompose(y, model='additive')\n",
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "#print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "#print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "#print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "#print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "#print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            #results = mod.fit()\n",
    "            \n",
    "            #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 1, 0, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "\n",
    "pred = results.get_prediction(start=pd.to_datetime('2019-01-01'), dynamic=False) \n",
    "pred_ci = pred.conf_int() \n",
    "\n",
    "y_forecasted = pred.predicted_mean\n",
    "y_truth = y['2019-01-01':]\n",
    "mse = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n",
    "\n",
    "pred_uc = results.get_forecast(steps=627) # predict the number in the next 640 days\n",
    "pred_ci = pred_uc.conf_int() \n",
    "a_pd = pd.DataFrame(pred_uc.predicted_mean)\n",
    "a_pd.reset_index(level=None, drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create and style traces\n",
    "fig.add_trace(go.Scatter(x=covid['date'], y=covid['deathNew'], name = 'New death cases',\n",
    "                        line=dict(color='#7f7f7f', width=1)))\n",
    "fig.add_trace(go.Scatter(x=a_pd['index'], y=a_pd['predicted_mean'], name = 'Predicted Accident(Daily)',\n",
    "                        line=dict(color='royalblue', width=1, dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=aa['Date'], y=aa['Accidents'], name='Actual Accident(Daily)',\n",
    "                        line=dict(color='firebrick', width=2)))\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "   xaxis=dict(\n",
    "       rangeselector=dict(\n",
    "           buttons=list([\n",
    "               dict(count=1,\n",
    "                    label=\"1m\",\n",
    "                    step=\"month\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(count=6,\n",
    "                    label=\"6m\",\n",
    "                    step=\"month\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(count=1,\n",
    "                    label=\"YTD\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"todate\"),\n",
    "               dict(count=1,\n",
    "                    label=\"1y\",\n",
    "                    step=\"year\",\n",
    "                    stepmode=\"backward\"),\n",
    "               dict(step=\"all\")])),\n",
    "       rangeslider=dict(visible=True),\n",
    "       type=\"date\"))\n",
    "\n",
    "fig.update_layout(title='Daily Actual Number and Predicted Number of Accidents')\n",
    "fig.update_yaxes(title='Number of Accidents')\n",
    "fig.update_xaxes(title='Date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b983b",
   "metadata": {},
   "source": [
    "**New death cases and actual accidents show an inverse variation relationship.**\n",
    "\n",
    "From April 2020-August 2021, we can see four cycles of New death cases and actual accidents interacting up and down. When New death cases decrease, real daily accidents increase, and vice versa. The convexity of the two curves is always opposite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e10eb1",
   "metadata": {},
   "source": [
    "## 3.6 Random Forest: Speed limits has the greatest impact on traffic accidents\n",
    "\n",
    "Here we extract all the variables used above to make more accurate predictions about the factors that affect the severity of traffic accidents using variables such as driver age, junction detail, weather conditions as well as travel time and speed limit conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e58c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weather_conditions']=df['weather_conditions'].apply(str)\n",
    "df['weather_conditions'].replace(['1.0','2.0','3.0','4.0','5.0','6.0','7.0','8.0','9.0','-1.0'],\n",
    "                                   ['Fine','Raining','Snowing',\n",
    "                   'Fine + high winds','Raining + high winds','Snowing + high winds',\n",
    "                   'Fog or mist','Other','Unknown','Data missing',]\n",
    "                                   ,inplace=True)\n",
    "\n",
    "num_col = ['speed_limit']\n",
    "col1=['Daytime','weather_conditions','junction_control','age_band_of_driver'\n",
    "      ,'junction_detail','road_type','sex_of_driver']\n",
    "targetcol=['accident_severity']\n",
    "cols = col1+targetcol+num_col\n",
    "\n",
    "model = df[cols].copy()\n",
    "#model.head()\n",
    "\n",
    "# create dummy variables from the categorical features\n",
    "dummies = pd.get_dummies(model[col1], drop_first=True)\n",
    "model = pd.concat([model[num_col], model[targetcol], dummies], axis=1)\n",
    "#check NaN\n",
    "print('number of NaN:',model.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3e6d4",
   "metadata": {},
   "source": [
    "**Step 1:** As we are using a tree-based model here, rather than a distance-based one, we can handle different ranges of features. Therefore, scaling is not required. We use the train_test_split function to select train data and test data from the sample at a random proportion (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b662a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.drop(['accident_severity'],axis=1) #define feature\n",
    "\n",
    "target = model[['accident_severity']] #define target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb9f76",
   "metadata": {},
   "source": [
    "**Step 2:** The advantages of the random forest algorithm in terms of classification effectiveness are its high classification accuracy, low generalisation error and ability to handle high-dimensional data, and the advantages of the training process are the fast learning process and the ease of parallelisation. However, when the distribution of data categories is unbalanced, i.e. the number of sample instances in one category is much smaller than the number of samples in the other categories, the random forest algorithm will have a series of problems such as poor classification results and large generalisation errors. So we need to further check the distribution of data categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking imbalanced classes\n",
    "model['accident_severity'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f3e82",
   "metadata": {},
   "source": [
    "**Step 3:** As we can see, we obtained a target with highly imbalanced classes, so we can't apply the best strategy and can't collect more data, especially from minority class. For this case we can use model evaluation metrics that are more appropriate for the unbalanced class: confusion matrix, precision or ROC curve instead of accuracy. Alternatively, we can use the class weight parameter included in some implementations of the model, which allows us to have the algorithm adjust for imbalanced classes. Here we focus on class weight parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_1 = RandomForestClassifier(random_state=4, criterion='entropy', n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "# train\n",
    "forest_1.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# predict\n",
    "y_test_preds  = forest_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "report = classification_report(y_test, y_test_preds)\n",
    "print('Classification Report Random Forest - with Entropy and class_weight Parameter: \\n', report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, y_test_preds)\n",
    "\n",
    "# create dataframe\n",
    "class_names = model.accident_severity.values\n",
    "dataframe = pd.DataFrame(matrix, index=['Fatal', 'Serious', 'Slight'], \n",
    "                         columns=['Fatal', 'Serious', 'Slight'])\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout(), plt.xlabel('True Values'), plt.ylabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118e4dc",
   "metadata": {},
   "source": [
    "**Step 4:** We found that random forests using the weight_class parameter did not perform very well in classifying severity levels. Therefore, let us try a resampling strategy to properly handle our unbalanced target classes. Synthetic Minority Over-sampling Technique (SMOTE). Here, we repeatedly sample from a minority class and replace it so that it is equal in size to the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec638d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)\n",
    "\n",
    "forest_2 = RandomForestClassifier(random_state=4, criterion='entropy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f518ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "forest_2.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# predict\n",
    "y_test_preds = forest_2.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat matrix\n",
    "matrix = confusion_matrix(y_test, y_test_preds)\n",
    "\n",
    "# create dataframe\n",
    "class_names = model.accident_severity.values\n",
    "dataframe = pd.DataFrame(matrix, index=['Fatal', 'Serious', 'Slight'], \n",
    "                         columns=['Fatal', 'Serious', 'Slight'])\n",
    "\n",
    "\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout(), plt.xlabel('True Values'), plt.ylabel('Predicted Values')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc29c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(forest_2.feature_importances_, index=features.columns)\n",
    "feat_importances.nlargest(10).sort_values().plot(kind='barh', color='pink', figsize=(10,5))\n",
    "plt.xlabel('Relative Feature Importance with Random Forest');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de640d",
   "metadata": {},
   "source": [
    "In the final result, the relative characteristics of the speed limit are of the highest importance, with office hours second. In real life, excessive driving speeds are often a direct cause of traffic accidents. \n",
    "Fine and rainy days have comparatively high relative feature importance in the weather indicator. In the 24 hours of the day, office hours and evening hour traffic accidents are more important.\n",
    "For policymakers, reducing the severity of traffic accidents can also start with limiting speeds and taking care to divert traffic during peak periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61614bc4",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61deea87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449b5c49",
   "metadata": {},
   "source": [
    "In this project, we first introduced the background and dataset of the study. And we downloaded the data of Road safety and Covid-19 through API and URL. Meanwhile, we loaded the UK-wide base map with the help of the API of Local Authority Districts. Finally, we make the following conclusions.\n",
    " \n",
    "1. Temporal distribution: In 2016-2020, the total number of accidents is highest at the end of each year, with most traffic accidents occurring during the morning and evening rush hours on weekdays.\n",
    "\n",
    "2. Sex: Unlike the traditional impression, males (2.94%) are more likely to be involved in traffic accidents than females (1.48%), so male drivers are twice as likely as female drivers to be involved in a traffic accident. And 70% of the casualties were men.\n",
    "\n",
    "3. Age: Thirty-year-olds are most likely to be involved in traffic accidents and become victims. However, after this time, the number of accidents gradually decreases. The age of 16-18 is another cut-off point that signals a significant increase in the likelihood of a traffic accident when reaching the age of legally obtaining driving and alcohol permission. Thus, we would suggest that - young people who have just received their driver's licence need to stay alert.\n",
    "\n",
    "4. Weather Condition: Serious and fatal accidents are more likely to occur in foggy, mist, and high winds. Serious traffic accidents are more likely to occur on sunny days than rainy or snowy days. So, drivers need to stay calm on sunny days as that old saying tells us - Sing before breakfast, you'll cry before supper.\n",
    "\n",
    "5. Junction condition: 58% of accidents occur at intersections, 50% of which occur at T or staggered junctions; stop signs and authorised persons can help reduce the number of casualties. From the driver's perspective, before passing through a traffic intersection, its supposed to maintain a high concentration level to avoid traffic accidents. From the policy maker's perspective, stop signs and signals should be installed at intersections where accidents are likely to occur. If possible, you can hire an authorised personnel to manage the traffic at the meeting.\n",
    "\n",
    "6. Spatial Analysis: Two largest cities in the UK - London and Birmingham have the highest traffic accidents. The traffic accident cluster is most remarkable near Liverpool Street Station and Monument Station in central London.\n",
    "\n",
    "7. Covid and road safety: In March 2020, the Covid-19 epidemic spread in the UK. After the occurrence of the Covid, the traffic accidents accelerated to decrease. New death cases and actual accidents show an inverse variation relationship.\n",
    "\n",
    "8. Random Forest: among all the indexes in the road safety dataset, the speed limit has the highest relative feature importance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484cbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df[['accident_severity']]\n",
    "a.columns = ['Type']\n",
    "b=df[['road_type']]\n",
    "b.columns = ['Type']\n",
    "c=df[['junction_control']]\n",
    "c.columns = ['Type']\n",
    "d=df[['junction_detail']]\n",
    "d.columns = ['Type']\n",
    "e=df[['Daytime']]\n",
    "e.columns = ['Type']\n",
    "\n",
    "word = pd.concat([a,b,c,d,e],axis=0)\n",
    "\n",
    "text = \" \".join(i for i in word.Type)\n",
    "#print (\"There are {} words in the combination of all cells in column BLOOM.\".format(len(text)))\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\",scale = 4,\n",
    "               width=2500, height=1000).generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.tight_layout(pad=0)\n",
    "plt.imshow(wordcloud\n",
    "           ,interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825dbee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd883ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287.9891357421875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
